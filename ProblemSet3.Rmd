---
title: "Problem Set 3"
author: "Kerem Tuncer"
date: "10/07/2021"
output:
  word_document: default
  html_notebook: default
---

```{r}
rm(list = ls())
```


## Question 1

### Part A

The standard error is a statistic that characterizes the amount of sampling variability. To be more precise, the standard error is the standard deviation of a sampling distribution, which is the collection of estimates that could have been generated by every possible random assignment. Likewise, the standard error shows us how far an estimate will likely be from that estimateâ€™s mean in a sampling distribution. Therefore, a sampling distribution with small standard error will usually provide precise estimates of an estimator.

On the other hand, the standard deviation is a description of how much our measurements vary. In more technical terms, it is a measure that shows how much the individual measurements in a sample vary from the sample mean. It basically tells us how scattered out our data points are.

There are two primary differences between the two. Firstly, the standard deviation describes variability within a single sample, while standard error describes variability across the sampling distribution. Secondly, standard deviation is a descriptive statistic that can be computed from the data we have, whereas standard error is an inferential statistic that we can only estimate.

### Part B

Let's first begin by defining what the sharp null hypothesis of no effect means. This hypothesis indicates assumes that the treatment effect is zero for all subjects. This null hypothesis allows us to formally link the observed data to all potential outcomes. In other words, it allows us to fill in the missing potential outcomes of our data. For example, if a unit was assigned to the treatment and had an outcome of 2, then the sharp null hypothesis allows us to assume that the potential untreated outcome would also be 2. With this assumption, we can conduct a randomization inference, a test where we compute the sampling distribution of the estimated average treatment effect under the sharp null hypothesis by simulating all possible random assignments. It is important to mention once again that the sharp null hypothesis is what allows us to have both potential outcomes for simulating all the random assignments. Furthermore, the good thing about randomization inference is that it provides an exact sampling distribution of the estimated average treatment effect under the sharp null hypothesis.

### Part C

The 95% confidence interval is an interval that has a 0.95 probability of bracketing the true average treatment effect. So, if we were to replicate a  study under identical conditions, 95 out of 100 random assignments will generate intervals that bracket the true ATE. It is important to note that the 95% confidence interval is a random variable, because the location of the interval will vary from one experiment to the next, depending on the specific random assignment.

### Part D

Let's revisit what complete random assignment is. It is a randomization procedure where exactly m of N units are assigned to the treatment group with equal probability. On the other hand, block random assignment is a procedure where subjects are partitioned into subgroups (called blocks or strata), and complete random assignment occurs within each block. Hence, the primary difference between complete random assignment and block random assignment is that, in the former, the participants are not divided into subgroups while randomization is done, whereas, in the latter, the participants are already divided in the subgroups within which randomization takes place.

On the other hand, under cluster assignment, all subjects in the same cluster are placed as a group into either the treatment or control condition; in effect, cluster assignment rules out all of the possible allocations in which individuals in the same cluster are assigned to different experimental conditions. The primary difference between complete random assignment and cluster assignment is that, in the latter, the random assignment is done for groups (so, all subjects in a group will be in that experimental condition), while, in the former, the random assignment is done for each subject.

Likewise, the footnotes of the textbook also state that a defining feature of complete (as opposed to clustered or blocked) random assignment is that all possible assignments of N subjects to a treatment group of size m are equally likely.

### Part E

One advantage of balanced designs is that regression adjustment does not produce bias when the treatment and control groups are of equal size and treatment effects are constant (regardless of sample size). Thus, for balanced designs, regression adjustment is unbiased under the sharp null hypothesis of no effect for any subject.

A second advantage is that a balanced design will make it more likely to have a smaller standard error. An implication of the SE formula is that when the variances of Yi(0) and Yi(1) are similar, it is advisable to assign approximately half of the observations to the
treatment group, such that m is approximately equal to N/2. In practice, researchers seldom know in advance which group is likely to have more variance; therefore, it is more appropriate to place equal numbers of subjects in each condition to get a smaller estimated SE.

Lastly, assigning an equal number of subjects to treatment and control facilitates interval estimation by eliminating complications, such as having a too wide / too narrow interval depending on whether Var(Yi(0)) is larger or smaller than Var(Yi(1)). It is worth mentioning that this complication can occur specifically when estimating the interval with the method where we add/subtact the estimated ATE to impute for potential outcomes, and then look at the 2.5th and 97.5th percentile marks for creating the interval.


## Question 2

This is the equation.

$$\sqrt{\frac{1}{N-1}(\frac{mVar(Y_i(0))}{N-m}+ \frac{(N-m)Var(Y_i(1))}{m} + 2(Cov(Y_i(0), Y_i(1))))}$$

$$\sqrt{\frac{1}{2m-1}(\frac{mVar(Y_i(0))}{m}+ \frac{mVar(Y_i(0)+\tau_i)}{m} + 2(Cov(Y_i(0), Y_i(0)+\tau_i)))}$$
$$\sqrt{\frac{1}{2m-1}(\frac{mVar(Y_i(0))}{m}+ \frac{mVar(Y_i(0)+\tau_i)}{m} + 2(Cov(Y_i(0), Y_i(0)+\tau_i)))}$$
$$\sqrt{\frac{1}{2m-1}(Var(Y_i(0))+ Var(Y_i(0)+\tau_i) + 2(Cov(Y_i(0), Y_i(0)+\tau_i)))}$$

$$\sqrt{\frac{1}{2m-1}(Var(Y_i(0))+ Var(Y_i(0)+\tau_i) + 2(Cov(Y_i(0), Y_i(0)+Cov(Y_i(0),\tau_i))))}$$

$$\sqrt{\frac{1}{2m-1}(Var(Y_i(0))+ Var(Y_i(0)+\tau_i) + 2(Var(Y_i(0)+Cov(Y_i(0),\tau_i))))}$$

$$\sqrt{\frac{1}{2m-1}(Var(Y_i(0))+ Var(Y_i(0))+Var(\tau_i) + 2Cov(Y_i(0),\tau_i) + 2(Var(Y_i(0)+Cov(Y_i(0),\tau_i))))}$$

$$\sqrt{\frac{1}{2m-1}(2Var(Y_i(0))+Var(\tau_i) + 2Cov(Y_i(0),\tau_i) + 2Var(Y_i(0))+2Cov(Y_i(0),\tau_i))}$$

$$\sqrt{\frac{1}{2m-1}(4Var(Y_i(0))+Var(\tau_i) + 4Cov(Y_i(0),\tau_i))}$$

From the above equation, we can come to a total of four conclusions. Firstly, given that the sample size is in the denominator, we can say that a larger sample size will lead to a smaller standard error. Secondly, a larger variance in Yi(0) will lead to a larger standard error. Thirdly, a larger variance in $\tau_i$ will lead to a a larger standard error. Fourthly, a larger covariance between Yi(0) and $\tau_i$ will lead to a larger standard error.


## Question 3

$$Var(Y_i(0)) = Var(Y_i(1))$$
$$Var(Y_i(0)) = Var(Y_i(0 )+ \tau_i)$$

If the treatment effects are the same, then $\tau_i$ is a constant, then we can denote it as $\tau$ instead of $\tau_i$.

$$Var(Y_i(0)) = Var(Y_i(0 )+ \tau)$$
Given that $\tau$ is a constant, we can delete it.

$$Var(Y_i(0)) = Var(Y_i(0))$$
Therefore, we can say that $Var(Y_i(0)) = Var(Y_i(1))$ when the treatment effects are the same for all subjects.

Now, let's take a look at the correlation formula.

$$\frac{Cov(Y_i(0),Y_i(1))}{\sqrt{Var(Y_i(0))Var(Y_i(1))}}$$
$$\frac{Cov(Y_i(0),Y_i(1))}{\sqrt{Var(Y_i(0))Var(Y_i(0)+\tau_i)}}$$
If the treatment effects are the same, then $\tau_i$ is a constant, then we can denote it as $\tau$ instead of $\tau_i$.

$$\frac{Cov(Y_i(0),Y_i(1))}{\sqrt{Var(Y_i(0))Var(Y_i(0)+\tau)}}$$
Given that $\tau$ is a constant, we can delete it.

$$\frac{Cov(Y_i(0),Y_i(1))}{\sqrt{Var(Y_i(0))Var(Y_i(0))}}$$
$$\frac{Cov(Y_i(0),Y_i(1))}{\sqrt{Var(Y_i(0))^2}}$$
$$\frac{Cov(Y_i(0),Y_i(1))}{Var(Y_i(0))}$$
$$\frac{Cov(Y_i(0),Y_i(0)+\tau)}{Var(Y_i(0))}$$
$$\frac{Var(Y_i(0))+Cov(Y_i(0),\tau)}{Var(Y_i(0))}$$
The covariance of a constant with a random variable is zero, so we can delete the covariance term.

$$Cov(Y_i(0),\tau) = 0$$


$$\frac{Var(Y_i(0))}{Var(Y_i(0))} = 1$$
## Question 4

### Part A

This is the equation.

$$\sqrt{\frac{1}{N-1}(\frac{mVar(Y_i(0))}{N-m}+ \frac{(N-m)Var(Y_i(1))}{m} + 2Cov(Y_i(0), Y_i(1)))}$$

$$\sqrt{\frac{1}{4}(\frac{3Var(Y_i(0))}{2}+ \frac{2Var(Y_i(1))}{3} + 2Cov(Y_i(0), Y_i(1)))}$$

Now, let's compute.

```{r}
control <- c(1,2,3,4,5)
treat.a <- c(2,3,4,5,6)
treat.b <- rep(3,5)

var.c <- var(control)
var.a <- var(treat.a)
var.b <- var(treat.b)

cov.a <- cov(control, treat.a)
cov.b <- cov(control, treat.b)
```

```{r}
se.a <- sqrt(0.25 * ((3 * var.c / 2)+ (2 * var.a / 3)+ (2 * cov.a)))
se.b <- sqrt(0.25 * ((3 * var.c / 2)+ (2 * var.b / 3)+ (2 * cov.b)))

se.b < se.a
se.a
se.b
```

The standard error with treatment A is 1.61 and with treatment B is 0.97. Therefore, treatment B will generate a sampling distribution with a smaller standard error.

### Part B

The result suggests that a treatment that will produce a potential treated outcome with smaller variances will be better at closing an achievement gap. This is because the potential treated outcomes will have a narrower spread/distribution, so the achievement gap will be narrower, as well.

## Question 5

### Part A

```{r}
control_n <- 6
control_total <- 10 + 15 + 20 + 20 + 10 + 15 + 15
```

Village 1 in control

```{r}
treatment.mean <- 15
control.mean <- (control_total - 10) / control_n
ate.1 <- treatment.mean - control.mean
ate.1
```
The estimate difference-in-means is -0.83333 when village 1 is in control.

Village 2 in control

```{r}
treatment.mean <- 15
control.mean <- (control_total - 15) / control_n
ate.2 <- treatment.mean - control.mean
ate.2
```
The estimate difference-in-means is 0 when village 2 is in control.

Village 3 in control

```{r}
treatment.mean <- 30
control.mean <- (control_total - 20) / control_n
ate.3 <- treatment.mean - control.mean
ate.3
```

The estimate difference-in-means is 15.83333 when village 3 is in control.

Village 4 in control

```{r}
treatment.mean <- 15
control.mean <- (control_total - 20) / control_n
ate.4 <- treatment.mean - control.mean
ate.4
```
The estimate difference-in-means is 0.83333 when village 4 is in control.

Village 5 in control

```{r}
treatment.mean <- 20
control.mean <- (control_total - 10) / control_n
ate.5 <- treatment.mean - control.mean
ate.5
```
The estimate difference-in-means is 4.16667 when village 5 is in control.

Village 6 in control

```{r}
treatment.mean <- 15
control.mean <- (control_total - 15) / control_n
ate.6 <- treatment.mean - control.mean
ate.6
```
The estimate difference-in-means is 0 when village 6 is in control.

Village 7 in control

```{r}
treatment.mean <- 30
control.mean <- (control_total - 15) / control_n
ate.7 <- treatment.mean - control.mean
ate.7
```
The estimate difference-in-means is 15 when village 7 is in control.

### Part B

```{r}
mean(c(ate.1, ate.2, ate.3, ate.4, ate.5, ate.6, ate.7))
```

The average of these estimates is 5. From Table 2.1, we can see that the true ATE was 5. So, they are equal.

### Part C

```{r}
sd(c(ate.1, ate.2, ate.3, ate.4, ate.5, ate.6, ate.7))
```
The standard deviation of the seven estimates was 7.297.

```{r}
control <- c(10, 15, 20, 20, 10, 15, 15)
treat <- c(15, 15, 30, 15, 20, 15, 30)

var.c <- var(control)
var.t <- var(treat)

cov <- cov(control, treat)
```

```{r}
se <- sqrt((1/6) * ((1 * var.c / 6)+ (6 * var.t / 1)+ (2 * cov)))
se

se == sd(c(ate.1, ate.2, ate.3, ate.4, ate.5, ate.6, ate.7))
```

The standard error implied by equation 3.4 is also 7.297. So, the two values are the same.

### Part D

```{r}
se <- sqrt((1/6) * ((1 * var.c / 6)+ (6 * var.t / 1)+ (2 * cov)))
se

se.2 <- sqrt((1/6) * ((2 * var.c / 5)+ (5 * var.t / 2)+ (2 * cov)))
se.2

se > se.2

(1 * var.c / 6)
(6 * var.t / 1)
(2 * var.c / 5)
(5 * var.t / 2)
```

The design where one village is in the control group will end up producing $\frac{(N-m)Var(Y_i(1))}{m}$ that is very large. Therefore, The standard error of the one treatment study will be 7.30, while the standard error of the two treatment study will be 4.97, which is much lower.

### Part E

```{r}
se <- sqrt((1/6) * ((1 * var.c / 6)+ (6 * var.t / 1)+ (2 * cov)))
se

se.3 <- sqrt((1/6) * ((6 * var.c / 1)+ (1 * var.t / 6)+ (2 * cov)))
se.3

se > se.3

(1 * var.c / 6)
(6 * var.t / 1)
(6 * var.c / 1)
(1 * var.t / 6)
```

The design where one village is in the control group will end up producing $\frac{(N-m)Var(Y_i(1))}{m}$ that is very large. Therefore, The standard error of the one treatment study will be 7.30, while the standard error of the two treatment study will be 4.56, which is much lower.

## Question 6

```{r}
library(foreign)
hajj <- read.dta("Clingingsmith subset.dta")
```

```{r}
library(ri)
set.seed(1234567) 
Z <- as.integer(hajj$success) - 1   # convert treat to an indicator: 1=treat
Y <- hajj$views                   # sum of views toward various national groups
```

```{r}
probs <- genprobexact(Z)          # generate probability of treatment 

ate <- estate(Y,Z,prob=probs)     # estimate the ATE

perms <- genperms(Z,maxiter=10000)  # set the number of simulated random assignments

Ys <- genouts(Y,Z,ate=0)            # create potential outcomes UNDER THE SHARP NULL OF NO EFFECT FOR ANY UNIT

distout <- gendist(Ys,perms,prob=probs)  # generate the sampling distribution  based on the schedule of potential outcomes implied by the null hypothesis

ate                                 # estimated ATE
sum(distout >= ate)                 # one-tailed comparison used to calculate p-value (greater than)
sum(abs(distout) >= abs(ate))       # two-tailed comparison used to calculate p-value

dispdist(distout,ate)               # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null

```

```{r}
0.0039*10000
0.0019*10000
```

The estimated average treatment effect is 0.4748. The number of simulated ATEs that were at least as large as the actual estimated ATE is 19, corresponding to a p-value of 0.0019 for the one-tailed test. The number of simulated ATEs that were as large in absolute value as the actual estimated ATE is 39, corresponding to a p-value of 0.0039, for the two-tailed test.

## Question 7

```{r}
set.seed(1234567)
Z <- c(1,1,1,1,1,0,0,0,0,0)
Y <- c(-5, 4, 7, -7, -4, 1, 0, 0, 4, 3)    
```


```{r}
probs <- genprobexact(Z)          # generate probability of treatment 

ate <- estate(Y,Z,prob=probs)     # estimate the ATE

perms <- genperms(Z,maxiter=10000)  # set the number of simulated random assignments

Ys <- genouts(Y,Z,ate=0)            # create potential outcomes UNDER THE SHARP NULL OF NO EFFECT FOR ANY UNIT

distout <- gendist(Ys,perms,prob=probs)  # generate the sampling distribution  based on the schedule of potential outcomes implied by the null hypothesis

ate                                 # estimated ATE
sum(distout >= ate)                 # one-tailed comparison used to calculate p-value (greater than)
sum(abs(distout) >= abs(ate))       # two-tailed comparison used to calculate p-value

dispdist(distout,ate)               # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null

```


The sharp null hypothesis is that $\tau_i$ is equal to 7 for all i. The alternative hypothesis is that $\tau_i$ is greater than 7 for all i. A one-sided test is used here because we want to determine if there is a difference between the two groups in a specific direction. In our case, the direction we are interested in is that the weight loss will be greater than 7. The observed difference in weight loss between the treatment and control groups is -2.6, which is larger than 21% of all simulated experiments under the null hypothesis. Thus the p-value is 0.206, meaning we cannot reject the null hypothesis at the conventional 0.05 significance threshold.

## Question 8

### Part A

```{r}
titiunik <- read.dta("Titiunik data for Exercises to Chapter 3.dta")
```


```{r}
ate.texas <- mean(titiunik$bills_introduced[titiunik$term2year == 1 & titiunik$texas0_arkansas1 == 0]) - mean(titiunik$bills_introduced[titiunik$term2year == 0 & titiunik$texas0_arkansas1 == 0])
ate.texas

ate.arkansas <- mean(titiunik$bills_introduced[titiunik$term2year == 1 & titiunik$texas0_arkansas1 == 1]) - mean(titiunik$bills_introduced[titiunik$term2year == 0 & titiunik$texas0_arkansas1 == 1])
ate.arkansas
```

The estimated average treatment effect is -16.74 for Texas and -10.094 for Arkansas.

### Part B

```{r}
se.texas <- sqrt(
  (sd(titiunik$bills_introduced[titiunik$term2year == 1 & titiunik$texas0_arkansas1 == 0])^2 / 
     length(titiunik$bills_introduced[titiunik$term2year == 1 & titiunik$texas0_arkansas1 == 0])) +
    (sd(titiunik$bills_introduced[titiunik$term2year == 0 & titiunik$texas0_arkansas1 == 0])^2 / 
       length(titiunik$bills_introduced[titiunik$term2year == 0 & titiunik$texas0_arkansas1 == 0]))
)
se.texas

se.arkansas <- sqrt(
  (sd(titiunik$bills_introduced[titiunik$term2year == 1 & titiunik$texas0_arkansas1 == 1])^2 / 
     length(titiunik$bills_introduced[titiunik$term2year == 1 & titiunik$texas0_arkansas1 == 1])) +
    (sd(titiunik$bills_introduced[titiunik$term2year == 0 & titiunik$texas0_arkansas1 == 1])^2 / 
       length(titiunik$bills_introduced[titiunik$term2year == 0 & titiunik$texas0_arkansas1 == 1]))
)

se.arkansas
```

The estimated standard error of the estimated ATE is 9.346 for Texas and 3.396 for Arkansas.

### Part C

```{r}
n.texas <- length(titiunik$texas0_arkansas1[titiunik$texas0_arkansas1 == 0])
n.arkansas <- length(titiunik$texas0_arkansas1[titiunik$texas0_arkansas1 == 1])
n <- nrow(titiunik)

ate.texas*(n.texas/n) + ate.arkansas*(n.arkansas/n)
```
The combined ATE is -13.2168.

### Part D

```{r}
prob.n.texas <- length(titiunik$texas0_arkansas1[titiunik$texas0_arkansas1 == 0 & titiunik$term2year == 1])/length(titiunik$texas0_arkansas1[titiunik$texas0_arkansas1 == 0])
prob.n.texas

prob.n.arkansas <- length(titiunik$texas0_arkansas1[titiunik$texas0_arkansas1 == 1 & titiunik$term2year == 1])/length(titiunik$texas0_arkansas1[titiunik$texas0_arkansas1 == 1])
prob.n.arkansas
```


When the probability of being assigned to the treatment group varies by block, as it does in this study, comparing means for all subjects generates a biased estimate of the ATE. The probability of being assigned to treatment is 0.48 for the study in Texas and 0.51 for the study in Arkansas. Hence, the probabilities are different.

### Part E

```{r}
arkansas <- subset(titiunik, texas0_arkansas1 == 1)
texas <- subset(titiunik, texas0_arkansas1 == 0)
sqrt((se.texas)^2*(nrow(texas)/ nrow(titiunik))^2+(se.arkansas)^2*(nrow(arkansas)/ nrow(titiunik))^2)
```
The estimated standard error for the overall ATE is 4.74478.

### Part F

```{r}
titiunik <- read.dta("Titiunik data for Exercises to Chapter 3.dta")
Z <-  titiunik$term2year       # treatment is 2 year rather than 4 year term
Y <- titiunik$bills_introduced
block <- titiunik$texas0_arkansas1   # randomization occurs within each state

probs <- genprobexact(Z,blockvar=block)   # blocking is assumed when generating probability of treatment
table(probs)

ate <- estate(Y,Z,prob=probs)      # estimate the ATE

perms <- genperms(Z,maxiter=10000,blockvar=block)   # set the number of simulated random assignments


Ys <- genouts(Y,Z,ate=0)    # create potential outcomes under the sharp null of no effect for any unit

distout <- gendist(Ys,perms,prob=probs)  # generate the sampling distribution  based on the schedule of potential outcomes implied by the null hypothesis

ate                             # estimated ATE
mean(abs(distout) >= abs(ate))  # two-tailed comparison used to calculate p-value


dispdist(distout,ate)       # display p-value
```

The sharp null hypothesis is that $\tau_i$ is equal to 0 for all i. The alternative hypothesis is that $\tau_i$ is not equal to 0 for all i. A two-sided test is used here because the question does not specify whether the researchers think that two-year terms will increase or decrease the number of bills introduced. Hence, we are only interested in finding out if there is a difference between the bills introduced between two-year terms and four year-terms. We are not interested in the direction of the effect.

The two-sided p-value is 0.0083, which means that the number of simulated ATEs that were as large in absolute value as the actual estimated ATE is 83. Given that the p-value is 0.206, meaning we can reject the null hypothesis at the conventional 0.05 significance threshold. We have enough evidence to conclude that $\tau_i$ is not equal to 0 for all i.

## Question 9

### Part A

This design feature does not affect violate the non-interference assumption because the bets were done before the start of the race. Therefore, the race performance of the horses did not affect the betting odds of the other horse. The non-interference assumption would have been violated if the betting amount outcome was collected at a time after the start of the race. This is why Camerer defined the potential outcome to be the betting amount during the test period (which is before the race starts).

### Part B

```{r}
camerer <- read.dta("Camerer data for Chapter 3 exercises.dta")
covs <- as.matrix(camerer$preexperimentbets)
Z <- camerer$treatment
block <- camerer$pair


numiter <- 10000

perms <- genperms(Z,maxiter=numiter,blockvar=block)
numiter <- ncol(perms)  # reset numiter so that it is no larger than the maximum number of possible randomizations

## Use F-test to assess the null hypothesis that the covariates predict random assignment (Z) no better than would be expected by chance

Fstat <- summary(lm(Z~covs))$fstatistic[1]   # F-statistic from actual data

Fstatstore <- rep(NA,numiter)

for (i in 1:numiter) {
	Fstatstore[i] <- summary(lm(perms[,i]~covs))$fstatistic[1]  # F-statistic under the null of random assignment of Z
	}

mean(Fstatstore >= Fstat)           

```

We have conducted a F-test, which is a regression of treatment assignment on pre-experimental bets controlling for blocks to see if excludability was violated. In other words, we are checking to see whether the covariate (which was pre-experimental bet) had an effect on whether a horse ended up in the treatment or the control group. In the f-test, our null hypothesis would be that the covariates are approximately equal between the two experimental conditions, and our alternative hypothesis is that they are not equal. Given that our p-value is 0.3615, we fail to reject the null hypothesis at the 0.05 level. Therefore, we can conclude that there is no significant covariate imbalance between the experimental conditions.

### Part C

```{r}
t.mean <- mean(camerer$experimentbets[camerer$treatment == 1])
c.mean <- mean(camerer$experimentbets[camerer$treatment == 0])
t.mean
c.mean
ate = t.mean - c.mean
ate
```

The mean outcome for the treatment group was 461.24 and the mean outcome for the control group was 571.41. The estimated average treatment effect is -110.1765. Given that the ATE is negative, we can say that the treatment caused a decrease in the bet amount by 110.1765 units.

### Part D

```{r}
treatment <- subset(camerer, treatment == 1)
control <- subset(camerer, treatment == 0)
colnames(control) <- c("pair", "preexperimentbets", "experimentbets.control", "treatment")
combined <- cbind(treatment, control)

combined$difference <- combined$experimentbets - combined$experimentbets.control

ate.2 <- mean(combined$difference)
ate.2
```
Therefore, the estimated ATEs are the same with both methods.

### Part E

```{r}

Z <-  camerer$treatment
Y <- camerer$experimentbets
block <- camerer$pair        # indicates how the subjects are blocked      

# RI for the effect of the covariate on the treatment

covs <- as.matrix(camerer$preexperimentbets)     # reads in covariates

probs <- genprobexact(Z,blockvar=block)

numiter <- 10000

perms <- genperms(Z,maxiter=numiter,blockvar=block)
numiter <- ncol(perms)  

probs <- genprobexact(Z,blockvar=block)    # notice the use of block to indicate how the random assignment was conducted

ate <- estate(Y,Z,prob=probs)

perms <- genperms(Z,maxiter=10000,blockvar=block)    # simulated block random assignments

Ys <- genouts(Y,Z,ate=0)

distout <- gendist(Ys,perms,prob=probs)  # generate the sampling distribution  based on the schedule of potential outcomes implied by the null hypothesis

ate
sum(distout <= ate)
sum(abs(distout) >= abs(ate))

dispdist(distout,ate)   # dis
```
The sharp null hypothesis is that $\tau_i$ is equal to 0 for all i. The alternative hypothesis is that $\tau_i$ is not equal to 0 for all i. A two-sided test is used here because the question does not specify whether the researchers think that the early bets will increase or decrease the total betting amount. Hence, we are only interested in finding out if there is a difference between the betting amount between treatment and control subjects. We are not interested in the direction of the effect.

The two-sided p-value is 0.3102, which means that the number of simulated ATEs that were as large in absolute value as the actual estimated ATE is 3102. Given that the p-value is 0.3102, meaning we cannot reject the null hypothesis at the conventional 0.05 significance threshold. We do not have enough evidence to conclude that $\tau_i$ is not equal to 0 for all i. In other words, we do not have enough evidence to conclude that the treatment made a difference in the betting amount outcome.

## Question 10

To solve this question we must first take a look at equation 3.22.

$$\sqrt{\frac{1}{k-1}(\frac{mVar(\overline{Y}_j(0))}{N-m}+ \frac{(N-m)Var(\overline{Y}_j(1))}{m} + 2(Cov(\overline{Y}_j(0), \overline{Y}_j(1))))}$$
In this equation we can change $\overline{Y}_j(0)$ and $\overline{Y}_j(1)$ with the following:

$$\frac{\sum_{i=1}^{N/k}Y_i(0)}{\frac{N}{k}}\quad and \quad \frac{\sum_{i=1}^{N/k}Y_i(1)}{\frac{N}{k}}$$

These two values above can be further simplifed like this:

$$\frac{k}{N}*\sum_{i=1}^{N/k}Y_i(0)\quad and \quad \frac{k}{N}*\sum_{i=1}^{N/k}Y_i(1)$$
We can plug these values back into equation 3.22.

$$\sqrt{\frac{1}{k-1}(\frac{m*\frac{k}{N}*Var(\sum_{i=1}^{N/k}Y_i(0))}{N-m}+ \frac{(N-m)*\frac{k}{N}*Var(\sum_{i=1}^{N/k}Y_i(1))}{m} + 2(Cov(\frac{k}{N}*\sum_{i=1}^{N/k}Y_i(0),\frac{k}{N}* \sum_{i=1}^{N/k}Y_i(1))))}$$

Then, we can take the common $\frac{k}{N}$ and put it in front of the equation with the parenthesis.

$$\sqrt{\frac{1}{k-1}*\frac{k}{N}*(\frac{m*Var(\sum_{i=1}^{N/k}Y_i(0))}{N-m}+ \frac{(N-m)*Var(\sum_{i=1}^{N/k}Y_i(1))}{m} + 2(Cov(\sum_{i=1}^{N/k}Y_i(0),\sum_{i=1}^{N/k}Y_i(1))))}$$
In the end, simplifying the above will get us this:

$$\sqrt{\frac{k}{(k-1)*N}*(\frac{m*Var(\sum_{i=1}^{N/k}Y_i(0))}{N-m}+ \frac{(N-m)*Var(\sum_{i=1}^{N/k}Y_i(1))}{m} + 2(Cov(\sum_{i=1}^{N/k}Y_i(0),\sum_{i=1}^{N/k}Y_i(1))))}$$
Now, let's take a look at equation 3.4:

$$\sqrt{\frac{1}{N-1}(\frac{mVar(Y_i(0))}{N-m}+ \frac{(N-m)Var(Y_i(1))}{m} + 2(Cov(Y_i(0), Y_i(1))))}$$

Between these two equations, I will compare $\frac{k}{(k-1)*N}$ and $\frac{1}{N-1}$.

We can say that:

$$\frac{k}{k-1}\approx1 \quad and \quad N \approx N-1$$

Therefore, we can say that:

$$\frac{k}{(k-1)*N} \approx \frac{1}{N-1}$$

Let's try plugging in our numbers. N will equal 800 and k will equal 800/25, which is 32.

$$\frac{32}{(32-1)*800} \approx \frac{1}{800-1}$$
$$\frac{32}{(31)*800} \approx \frac{1}{799}$$

```{r}
32/(31*800)
1/799
```

$$0.00129 \approx 0.00125$$
In conclusion, we would expect that this clustered design will produce approximately the same standard error as complete random assignment of individual students to treatment and control because the terms $\frac{k}{(k-1)*N}$ and $\frac{1}{N-1}$ will be very similar.

## Question 11

### Part A

```{r}
table3.3 <- read.csv("GerberGreenBook_Chapter3_Table_3_3.csv")
cluster.1 <- c(1,1,2,2,3,3,4,4,5,5,6,6,7,7)
cluster.2 <- c(1,2,3,4,5,6,7,7,6,5,4,3,2,1)
table3.3 <- cbind(table3.3, cluster.1, cluster.2)
```

```{r}
mean.control <- c(tapply(table3.3$Y, table3.3$cluster.1, mean))
mean.treatment <- c(tapply(table3.3$D, table3.3$cluster.1, mean))

var.control <- var(mean.control)
var.treatment <- var(mean.treatment)

covar <- cov(mean.control, mean.treatment)

N <- 14
k <- 7
m <- 4 * 2
N_m <- N - m
```

```{r}
sqrt((1/(k-1))*(((m*var.control)/(N_m)) + ((N_m*var.treatment)/(m)) + (2*covar)))
```

### Part B

I believe that this question has a wording error because it states that half of the clusters are assigned to treatment. Given that there are 7 clusters, we cannot really assign half to treatment. Instead, I will go with 4 clusters like we did in Part A. Regardless, of using 3 or 4 clusters, we will still end up with a lower standard error with the cluster formation method.

```{r}
mean.control <- c(tapply(table3.3$Y, table3.3$cluster.2, mean))
mean.treatment <- c(tapply(table3.3$D, table3.3$cluster.2, mean))

var.control <- var(mean.control)
var.treatment <- var(mean.treatment)

covar <- cov(mean.control, mean.treatment)

N <- 14
k <- 7
m <- 4 * 2
N_m <- N - m
```

```{r}
sqrt((1/(k-1))*(((m*var.control)/(N_m)) + ((N_m*var.treatment)/(m)) + (2*covar)))
```
### Part C

The villages 8-14 have both potential treated outcomes and potential untreated outcomes that are higher than those of villages 1-7. That is why, the first cluster formation method produced mean potential outcomes per cluster that had much higher variability because we matched villages that have low potential outcomes with villages that also have low potential outcomes (and we matched villages that have high potential outcomes with villages that also have high potential outcomes).

On the other hand, in the second cluster formation method, we matched villages that have low potential outcomes with villages that have high potential outcomes. Therefore, the mean potential outcomes per cluster were balanced out.

The implication for the design of cluster randomized experiments is that cluster formation can play a major role in producing a high or a low standard error. Therefore, if a researcher wants to have lower standard errors, he/she must form clusters in a way that the mean potential outcomes per cluster will be as close to each other as possible across all clusters. Unfortunately, in practice, this is hard to do, as researchers rarely have a say in how clusters are formed.

## Question 12

### Part A

```{r}
treatment <- c(0,1,1,2,2,2)
control <- rep(0,6)

ate <- mean(treatment) - mean(control)
ate
```
The average treatment effect is 1.3333.

### Part B

In this question, we can see that the clusters vary by size: A has one classroom, B has two classrooms, and C has three classrooms. Likewise, the number of clusters is too small to ensure that m of N units is assigned to treatment in each randomization. Let's say we wanted to have have half of the classrooms in the treatment. Then, a randomization where School A or School B ends up in the treatment will not allow us to have half of the units in treatment. Lastly, we can see how cluster size covaries with potential outcomes. The School A cluster has the smallest size and the smallest potential treated outcome. The School B cluster has the second smallest size and the second smallest potential treated outcome. The School C cluster has the largest size and the highest potential treated outcomes.

Given these pieces of information, we can say that the difference-in-means estimator is biased. Now, let's test these randomizations.
 
```{r}
treatment <- c(0)
control <- c(0,0,0,0,0)
ate.1 <- mean(treatment) - mean(control)

treatment <- c(1,1)
control <- c(0,0,0,0)
ate.2 <- mean(treatment) - mean(control)

treatment <- c(2,2,2)
control <- c(0,0,0)
ate.3 <- mean(treatment) - mean(control)

mean(c(ate.1, ate.2, ate.3))

ate == mean(c(ate.1, ate.2, ate.3))

```

As expected, the average of the three estimated average treatment effects is not equal to the true average treatment effect. The true ATE is 1.3333, whereas the average of the estimated ATEs is 1. Hence, the difference-in-means estimator was biased.

### Part C

```{r}
treatment <- c(0, 1, 1)
control <- c(0,0,0)
ate.new <- mean(treatment) - mean(control)

treatment <- c(2,2,2)
control <- c(0,0,0)
ate.new.2 <- mean(treatment) - mean(control)

mean(c(ate.new, ate.new.2))

mean(c(ate.new, ate.new.2)) == ate
```

When school A and B are combined, the difference-in-means estimator became unbiased because the average of the two estimated average treatment effects was equal to the true average treatment effect. They are both 1.3333.

It is worth mentioning that in Part C, combining school A and school B made it so that the clusters were the same size (they both had 3 schools). This implies that having clusters with equal sizes tends to produce unbiased estimates.

